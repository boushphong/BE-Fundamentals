# Content
- [Process vs Thread](#process-vs-thread)
- [How Connections are Established](#how-connections-are-established)
- [Reading and Sending Data](#reading-and-sending-data)
- [Listener, Acceptor, Reader](#listener-acceptor-reader)
- [Backend Idempotency](#backend-idempotency)

# Process vs Thread
## What is a Process?
- A set of instructions
- Has an isolated memory
- Has a PID
- Scheduled in the CPU

## What is a Thread?
- Light weight Process (LWP)
- A set of instructions
- Shares memory with parent process
- Has an ID
- Scheduled in the CPU

## Single Threaded Process
- One Process with a single thread
- Simple

## Multi-processes
- App has multiple processes
- Each has its own Memory
- Take advantage of multi-cores
- More memory but isolated
- Redis backup routine (COW)
- Examples: NGINX, Postgres

## Multi-Threaded
- One Process, multiple threads
- Shared Memory (compete)
- Take advantage of multi-cores
- Require less memory
- Race conditions
- Locks and Latches (SQL Server)
- Examples: Apache, Envoy

## How many is too many?
- Too many processes/threads
- CPU context switch
- Multiple Cores help
- Rule of thum -> Number of Cores = Number of Processes

# How Connections are Established
## Connection Establishment
- TCP Three-way handshake
- SYN/SYN-ACK/ACK

![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/ecdfc2c1-d5ff-4063-b568-d47cda000e3e)

- Server Listens on an address:port
- Client connects
- Kernel does the handshake creating a connection
- Backend process “Accepts” the connection

- Kernel creates a socket & two queues SYN and Accept
- Client sends a SYN
- Kernels adds to SYN queue, replies with SYN/ACK
- Client replies with ACK
- Kernel finish the connection
- Kernel removes SYN from SYN queue
- Kernel adds full connection to Accept queue
- Backend accepts a connection, removed from accept queue (Will be put in the application process)
- A file descriptor is created for the connection

![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/f05613c1-066c-4fad-a3fb-0d2cbe4c5e39)

The SYN queue and accept queue are both components of the TCP/IP stack in the kernel of an operating system, and they are closely related in the context of TCP connection establishment.

1. **SYN Queue**: When a client initiates a TCP connection by sending a SYN (synchronize) packet to the server, the server's TCP/IP stack processes the incoming SYN packets and queue it in the SYN Queue. The SYN queue holds pending connection requests that have not yet been fully established or accepted by the server. Once the connection is fully established via three-way handshake (SYN, SYN/ACK and ACK). The connection request will be moved to the accept queue. The job of the kernel here is done.

2. **Accept Queue**: The accept queue, also known as the connection backlog or listen backlog, holds fully established connections that are waiting to be accepted by the server's application. Once the server's application calls the `accept()` system call, it removes a connection request from the accept queue and creates a new socket for communication with the client. The accept queue allows the server to handle multiple pending connections concurrently, even when the server's application is busy processing other tasks.

**During the TCP connection establishment process:**

When the server's TCP/IP stack receives a SYN packet and successfully establishes a connection, it sends a SYN-ACK packet back to the client to acknowledge the connection request and to signal readiness to establish a connection. At this point, the connection is considered half-open, and the server's TCP/IP stack holds information about the pending connection in memory.

Once the server's application calls the `accept()` system call to accept the connection, the half-open connection transitions to a fully established connection, and the server's TCP/IP stack creates a new socket for communication with the client. This fully established connection is then moved from the SYN queue to the accept queue, where it waits to be processed by the server's application.

## Problems with accepting connections
- Backend doesn't accept fast enough
- Clients who don't ACK
- Small backlog
  - The server's OS may not immediately accept the connection but instead queues it up in a backlog
  - The server has a limited capacity to queue up pending connections. 

# Reading and Sending Data
## Send and receive buffers
- Client sends data on a connection
- Kernel puts data in receive queue
- Kernel ACKs (may delay) and update window
- App calls read to copy data

## Receive buffers
![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/00558d52-845c-4188-8659-9e48ee1d14b8)

## Send buffers
![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/85eebe2d-02f6-441c-8bba-03d135229e3a)

## Problems with reading and sending
- Backend doesn’t read fast enough
- Receive queue is full
- Client slows down

# Listener, Acceptor, Reader
1. **Listener**: A listener is a component or process that waits for incoming connections on a specific network port. When a client initiates a connection request to the server, the listener detects this request and triggers a corresponding action, such as accepting the connection or queuing it for later processing. In many programming frameworks or libraries, setting up a listener involves creating a socket and binding it to a specific port, then calling a function to start listening for incoming connections.

2. **Acceptor**: An acceptor is responsible for accepting incoming connection requests from clients. Once a connection request is detected by the listener, the acceptor performs the necessary steps to establish a connection with the client, such as creating a new socket for communication and initiating the TCP handshake process. In some programming environments, the acceptor may be a separate component or function from the listener, while in others, the listener may handle both listening and accepting connections.

3. **Reader**: A reader is a component or function that reads data from an established connection. Once a connection is established between the client and server, data can be exchanged between them. The reader is responsible for reading incoming data from the connection's input stream and processing it according to the application's logic. This may involve parsing incoming messages, performing data validation or transformation, and triggering appropriate actions based on the received data. In some cases, multiple readers may be used to handle concurrent connections or different types of data.

## Single Listener/Single Worker Thread
NodeJS has a single process and this process acts as the Listener, Acceptor and Reader altogether. All the workloads of listening, accepting and reading will be done asynchronously.

**Problems**:
- Cannot handle too many connections. The process will have to handle the management of all the connections.
  - Solution to this is to spin up more NodeJS processes.

![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/c9ecb971-28d8-469e-ba51-ae7c6872dd7a)

## Single Listener/Multiple Worker Threads
Memcached has a single process and this process will spin up multiple threads. This process acts as both the Listener and Acceptor. When connections in the kernel's queue are accepted by the main process, it put the connections in the process memory. And multiple threads have been spun up will be assigned a connection by the main process, and these threads will act as the readers, the reading will happen inside these threads.

**Problems**:
- Balancing workloads between reader threads. Some threads might accept more connections than others.

![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/a3779a37-ef5e-4016-8497-26f93718be99)


## Single Listener/Multiple Worker Threads with Load Balancing
Ramcloud has a single process and this process acts as the Listener, Acceptor and Reader altogether. Within the Ramcloud process, there are multiple worker threads. These worker threads are responsible for handling the incoming client requests concurrently. Each worker thread operates independently and is capable of processing one or more connections at the same time. By having multiple worker threads, the RamCloud instance can handle multiple client requests simultaneously, improving throughput and responsiveness.

**Load Balancing**: Load balancing mechanisms are employed to distribute incoming client connections among the available worker threads efficiently. When a new connection request is detected by the listener, it is handed off to one of the worker threads for processing. Load balancing algorithms ensure that connections are distributed evenly among the worker threads, preventing any single thread from becoming overwhelmed with too many connections.

**Request Processing**: Once a connection is established and handed off to a worker thread, it is responsible for processing the client's request. This may involve performing operations such as reading or writing data to the RamCloud storage, executing queries, or performing other tasks requested by the client. Each worker thread operates independently, allowing for concurrent processing of multiple client requests.

**Problems**:
- Single process doing all the listening, accepting and reading.
  - Solution to this is to spin up more processes, and distribute the listening, accepting and reading.

![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/9f5f8bfc-db2d-4616-8192-0b5530849f52)

## Multiple Threads Single Socket
NGINX has a single process and this process acts as the Listener. And there are multiple threads which act both as the Acceptor and Reader. However, with multple acceptor threads, all of these threads have access to the socket objects, and they will competively call `accept()` on the same object.

**Problems**:
- Competition of the accept queue.

![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/82dcb7bb-3091-4702-ae9b-bb0e1302aa34)

## Multiple Listeners on the same port

1. **Socket Sharding**: Normally, when a server application binds a socket to a specific port and starts listening for incoming connections, only one thread (or process) can listen on that port at a time. This means that all incoming connections are handled by the same thread, potentially leading to a bottleneck as the number of incoming connections increases.

2. **SO_REUSEPORT Socket Option**: The SO_REUSEPORT socket option is a feature available on some operating systems (such as Linux) that allows multiple sockets to bind to the same port simultaneously. When this option is enabled on a socket, multiple threads (or processes) can listen on the same port for incoming connections.

3. **Multiple Threads**: In a multi-threaded application, multiple worker threads are created to handle incoming connections. Each thread creates its own socket and enables the SO_REUSEPORT option on that socket. These sockets all bind to the same port, effectively allowing multiple threads to listen for incoming connections on the same port.

4. **Load Balancing**: With multiple threads listening on the same port, incoming connections are distributed among the threads using a load balancing mechanism provided by the operating system. When a new connection arrives, the operating system selects one of the listening threads to handle the connection. This load balancing ensures that the workload is evenly distributed among the threads, preventing any single thread from becoming overwhelmed.

5. **Concurrency**: Each listening thread operates independently, handling incoming connections concurrently. This allows the application to scale more effectively, as the workload is distributed across multiple threads. Additionally, because each thread operates independently, there is no contention for the port among the threads.
    
![image](https://github.com/boushphong/BE-Fundamentals/assets/59940078/6ef7dc92-9b48-458f-807f-10303a660876)

# Backend Idempotency
- Idempotent request can be retried without affecting backend
-  Implementation
  - Send a requestId with the request
    - If requestId has been processed return immediately (look up is required)
  - Upsert pattern
  - Idempotency token
 
## Backend Idempotency in HTTP
- GET is idempotent
- POST isn’t, but we can make it
- Browsers and proxies treat GET as idempotent
- Make sure your GETs are
